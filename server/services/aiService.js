const OpenAI = require('openai');
const { aiConfig, validateConfig } = require('../config/aiConfig');
const {
  withRetry,
  buildRequestParams,
  normalizeResponse,
  validateResponse
} = require('../utils/aiHelpers');
const logger = require('../utils/logger');
const memoryService = require('./memoryService');

const DEFAULT_LOCALE = 'zh';

const LOCALE_PROMPTS = {
  zh: {
    intro: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´åˆ›ä½œåŠ©æ‰‹ã€‚',
    responseDirective: 'è¯·ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ï¼Œå¹¶æ ¹æ®éœ€è¦å¯¹ç”¨æˆ·æä¾›çš„ä¿¡æ¯è¿›è¡Œæ¶¦è‰²ï¼Œä½¿å…¶è‡ªç„¶æµç•…ã€‚',
    novelInfoHeading: 'å½“å‰å°è¯´ä¿¡æ¯ï¼š',
    titleLabel: 'æ ‡é¢˜ï¼š',
    descriptionLabel: 'æè¿°ï¼š',
    genreLabel: 'ç±»å‹ï¼š',
    mainCharactersHeading: 'ä¸»è¦è§’è‰²ï¼š',
    worldSettingsHeading: 'ä¸–ç•Œè®¾å®šï¼š',
    contentRestrictionsLabel: 'å†…å®¹é™åˆ¶ï¼šåˆ†çº§',
    typeGuidance: {
      creative: 'è¯·ä»¥åˆ›æ„å’Œæƒ³è±¡åŠ›ä¸ºé‡ç‚¹å›ç­”ã€‚',
      analytical: 'è¯·ä»¥é€»è¾‘åˆ†æå’Œç»“æ„åŒ–æ€è€ƒä¸ºé‡ç‚¹å›ç­”ã€‚',
      consistency: 'è¯·é‡ç‚¹å…³æ³¨å†…å®¹çš„ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚',
      default: 'è¯·æä¾›æœ‰å¸®åŠ©çš„å»ºè®®å’Œåˆ†æã€‚'
    },
    memory: {
      heading: '=== ç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡ ===',
      intro: 'ä»¥ä¸‹æ˜¯ä¸å½“å‰å¯¹è¯ç›¸å…³çš„å†å²è®°å¿†ï¼Œè¯·åœ¨å›ç­”æ—¶å‚è€ƒè¿™äº›ä¿¡æ¯ä»¥ä¿æŒè¿è´¯æ€§å’Œä¸ªæ€§åŒ–ï¼š',
      closing: 'è¯·åŸºäºè¿™äº›è®°å¿†ä¿¡æ¯å’Œå½“å‰å°è¯´èƒŒæ™¯ï¼Œæä¾›è¿è´¯ã€ä¸€è‡´ä¸”ä¸ªæ€§åŒ–çš„å›ç­”ã€‚',
      conflictNotice: 'å¦‚æœå‘ç°è®°å¿†ä¸­çš„ä¿¡æ¯ä¸å½“å‰è®¾å®šæœ‰å†²çªï¼Œè¯·ä¼˜å…ˆä½¿ç”¨å½“å‰è®¾å®šå¹¶æé†’æˆ‘ã€‚',
      typeLabel: 'ç±»å‹',
      importantTag: 'é‡è¦'
    }
  },
  en: {
    intro: 'You are a professional novel writing assistant.',
    responseDirective: 'Always respond in English to match the current application language while keeping the tone clear and helpful.',
    novelInfoHeading: 'Current novel information:',
    titleLabel: 'Title: ',
    descriptionLabel: 'Description: ',
    genreLabel: 'Genre: ',
    mainCharactersHeading: 'Main characters:',
    worldSettingsHeading: 'World settings:',
    contentRestrictionsLabel: 'Content rating: ',
    typeGuidance: {
      creative: 'Focus on creativity and imagination in your response.',
      analytical: 'Prioritize logical analysis and structured thinking.',
      consistency: 'Keep the response focused on consistency and coherence.',
      default: 'Provide helpful suggestions and analysis.'
    },
    memory: {
      heading: '=== Related memory context ===',
      intro: 'The following memories are relevant to this conversation. Use them to keep replies coherent and personalized:',
      closing: 'Use these memories together with the current novel context to deliver coherent, consistent, and personalized answers.',
      conflictNotice: 'If the memories conflict with the current setup, prioritize the current setup and point out the discrepancy.',
      typeLabel: 'Type',
      importantTag: 'Important'
    }
  }
};

function normalizeLocale(locale) {
  if (!locale) return DEFAULT_LOCALE;
  const normalized = String(locale).toLowerCase();
  if (LOCALE_PROMPTS[normalized]) {
    return normalized;
  }
  const base = normalized.split('-')[0];
  return LOCALE_PROMPTS[base] ? base : DEFAULT_LOCALE;
}

function getLocaleConfig(locale) {
  return LOCALE_PROMPTS[normalizeLocale(locale)];
}

class AIService {
  constructor() {
    this.providers = new Map();
    this.initializeProviders();
    
    // éªŒè¯AIé…ç½®
    try {
      validateConfig();
    } catch (error) {
      console.warn('AIé…ç½®éªŒè¯è­¦å‘Š:', error.message);
    }
  }

  initializeProviders() {
    // Initialize OpenAI-compatible providers
    if (aiConfig.openai.apiKey) {
      this.providers.set('openai', {
        client: new OpenAI({
          apiKey: aiConfig.openai.apiKey,
          baseURL: aiConfig.openai.baseURL,
          timeout: aiConfig.openai.timeout
        }),
        type: 'openai',
        models: {
          chat: aiConfig.openai.model,
          embedding: aiConfig.openai.embeddingModel
        }
      });
    }

    // Initialize Claude provider
    if (aiConfig.claude.apiKey) {
      this.providers.set('claude', {
        client: null, // Will be initialized when needed
        config: {
          apiKey: aiConfig.claude.apiKey,
          baseURL: aiConfig.claude.baseURL,
          timeout: aiConfig.claude.timeout
        },
        type: 'claude',
        models: {
          chat: aiConfig.claude.model
        }
      });
    }

    // Initialize Gemini provider
    if (aiConfig.gemini.apiKey) {
      this.providers.set('gemini', {
        client: null, // Will be initialized when needed
        config: {
          apiKey: aiConfig.gemini.apiKey,
          baseURL: aiConfig.gemini.baseURL,
          timeout: aiConfig.gemini.timeout
        },
        type: 'gemini',
        models: {
          chat: aiConfig.gemini.model
        }
      });
    }

    // Support for custom OpenAI-compatible providers
    if (aiConfig.custom.name && aiConfig.custom.apiKey && aiConfig.custom.baseURL) {
      this.providers.set(aiConfig.custom.name, {
        client: new OpenAI({
          apiKey: aiConfig.custom.apiKey,
          baseURL: aiConfig.custom.baseURL
        }),
        type: 'openai',
        models: {
          chat: aiConfig.custom.model || 'gpt-3.5-turbo'
        }
      });
    }
  }

  getDefaultProvider() {
    const preferredProvider = aiConfig.global.defaultProvider;
    return this.providers.get(preferredProvider) || this.providers.values().next().value;
  }

  async chat(messages, options = {}) {
    const provider = options.provider ? this.providers.get(options.provider) : this.getDefaultProvider();
    
    if (!provider) {
      throw new Error('No AI provider available');
    }

    if (provider.type === 'openai') {
      return await this.openaiChat(provider, messages, options);
    } else if (provider.type === 'claude') {
      return await this.claudeChat(provider, messages, options);
    } else if (provider.type === 'gemini') {
      return await this.geminiChat(provider, messages, options);
    }

    throw new Error(`Unsupported provider type: ${provider.type}`);
  }

  async chatStream(messages, options = {}) {
    const provider = options.provider ? this.providers.get(options.provider) : this.getDefaultProvider();
    
    if (!provider) {
      throw new Error('No AI provider available');
    }

    if (provider.type === 'openai') {
      return await this.openaiChatStream(provider, messages, options);
    } else if (provider.type === 'claude') {
      return await this.claudeChatStream(provider, messages, options);
    } else if (provider.type === 'gemini') {
      return await this.geminiChatStream(provider, messages, options);
    }

    throw new Error(`Unsupported provider type: ${provider.type}`);
  }

  async openaiChat(provider, messages, options) {
    const taskType = options.taskType || 'default';
    const startTime = Date.now();

    const requestFn = async () => {
      const params = buildRequestParams('openai', taskType, {
        model: options.model || provider.models.chat,
        temperature: options.temperature,
        maxTokens: options.maxTokens,
        ...options.additionalParams
      });

      const requestData = {
        messages: messages,
        ...params
      };

      const response = await provider.client.chat.completions.create(requestData);

      // Log successful API call
      const duration = Date.now() - startTime;
      logger.logApiCall('openai', '/chat/completions', requestData, response, duration);

      return normalizeResponse(response, 'openai');
    };

    try {
      const result = await withRetry(requestFn, 'openai');

      return {
        content: result.content,
        usage: result.usage,
        provider: 'openai',
        model: result.model,
        finishReason: result.finishReason
      };
    } catch (error) {
      // Log failed API call
      const duration = Date.now() - startTime;
      const requestData = {
        model: options.model || provider.models.chat,
        messages: messages,
        temperature: options.temperature,
        max_tokens: options.maxTokens
      };
      logger.logApiCall('openai', '/chat/completions', requestData, null, duration, error);

      logger.error('OpenAI API Error:', {
        error: error.message,
        stack: error.stack,
        model: options.model || provider.models.chat,
        messageCount: messages.length
      });
      throw new Error(`OpenAI API Error: ${error.message}`);
    }
  }

  async claudeChat(provider, messages, options) {
    const startTime = Date.now();

    try {
      // Convert OpenAI format messages to Claude format
      const systemMessage = messages.find(m => m.role === 'system')?.content || '';
      const userMessages = messages.filter(m => m.role !== 'system');

      const requestData = {
        model: options.model || provider.models.chat,
        system: systemMessage,
        messages: userMessages,
        max_tokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
        ...options.additionalParams
      };

      const response = await fetch(`${provider.config.baseURL}/v1/messages`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': provider.config.apiKey,
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify(requestData)
      });

      if (!response.ok) {
        const errorText = await response.text();
        const duration = Date.now() - startTime;
        const error = new Error(`Claude API Error: ${response.status} ${response.statusText} - ${errorText}`);
        logger.logApiCall('claude', '/v1/messages', requestData, null, duration, error);
        throw error;
      }

      const data = await response.json();

      // Log successful API call
      const duration = Date.now() - startTime;
      logger.logApiCall('claude', '/v1/messages', requestData, data, duration);

      return {
        content: data.content[0].text,
        usage: data.usage,
        provider: 'claude'
      };
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error('Claude API Error:', {
        error: error.message,
        stack: error.stack,
        model: options.model || provider.models.chat,
        messageCount: messages.length
      });
      throw new Error(`Claude API Error: ${error.message}`);
    }
  }

  async geminiChat(provider, messages, options) {
    const startTime = Date.now();
    const taskType = options.taskType || 'default';

    try {
      // Convert OpenAI format messages to Gemini format
      const systemMessage = messages.find(m => m.role === 'system')?.content || '';
      const userMessages = messages.filter(m => m.role !== 'system');

      // Gemini uses "contents" array with "parts"
      const contents = userMessages.map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      }));

      // Add system instruction if present
      const requestData = {
        contents: contents,
        generationConfig: {
          temperature: options.temperature || 0.7,
          maxOutputTokens: options.maxTokens || 2048,
          topP: options.topP || 0.95,
          topK: options.topK || 40
        }
      };

      // Add system instruction if available
      if (systemMessage) {
        requestData.systemInstruction = {
          parts: [{ text: systemMessage }]
        };
      }

      const model = options.model || provider.models.chat;
      const url = `${provider.config.baseURL}/models/${model}:generateContent?key=${provider.config.apiKey}`;

      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestData),
        signal: options.signal
      });

      if (!response.ok) {
        const errorText = await response.text();
        const duration = Date.now() - startTime;
        const error = new Error(`Gemini API Error: ${response.status} ${response.statusText} - ${errorText}`);
        logger.logApiCall('gemini', `/models/${model}:generateContent`, requestData, null, duration, error);
        throw error;
      }

      const data = await response.json();

      // Log successful API call
      const duration = Date.now() - startTime;
      logger.logApiCall('gemini', `/models/${model}:generateContent`, requestData, data, duration);

      // Extract content from Gemini response
      const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';

      return {
        content: content,
        usage: {
          promptTokens: data.usageMetadata?.promptTokenCount || 0,
          completionTokens: data.usageMetadata?.candidatesTokenCount || 0,
          totalTokens: data.usageMetadata?.totalTokenCount || 0
        },
        provider: 'gemini',
        model: model,
        finishReason: data.candidates?.[0]?.finishReason || 'stop'
      };
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error('Gemini API Error:', {
        error: error.message,
        stack: error.stack,
        model: options.model || provider.models.chat,
        messageCount: messages.length
      });
      throw new Error(`Gemini API Error: ${error.message}`);
    }
  }

  async openaiChatStream(provider, messages, options) {
    const taskType = options.taskType || 'default';

    const params = buildRequestParams('openai', taskType, {
      model: options.model || provider.models.chat,
      temperature: options.temperature,
      maxTokens: options.maxTokens,
      stream: true, // Enable streaming
      ...options.additionalParams
    });

    const requestData = {
      messages: messages,
      ...params
    };

    try {
      const stream = await provider.client.chat.completions.create(requestData);
      return stream;
    } catch (error) {
      logger.error('OpenAI Stream Error:', {
        error: error.message,
        stack: error.stack,
        model: options.model || provider.models.chat,
        messageCount: messages.length
      });
      throw new Error(`OpenAI Stream Error: ${error.message}`);
    }
  }

  async claudeChatStream(provider, messages, options) {
    const startTime = Date.now();

    try {
      // Convert OpenAI format messages to Claude format
      const systemMessage = messages.find(m => m.role === 'system')?.content || '';
      const userMessages = messages.filter(m => m.role !== 'system');

      const requestData = {
        model: options.model || provider.models.chat,
        system: systemMessage,
        messages: userMessages,
        max_tokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
        stream: true, // Enable streaming
        ...options.additionalParams
      };

      const response = await fetch(`${provider.config.baseURL}/v1/messages`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': provider.config.apiKey,
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify(requestData)
      });

      if (!response.ok) {
        const errorText = await response.text();
        const error = new Error(`Claude Stream Error: ${response.status} ${response.statusText} - ${errorText}`);
        throw error;
      }

      return response.body;
    } catch (error) {
      logger.error('Claude Stream Error:', {
        error: error.message,
        stack: error.stack,
        model: options.model || provider.models.chat,
        messageCount: messages.length
      });
      throw new Error(`Claude Stream Error: ${error.message}`);
    }
  }

  async geminiChatStream(provider, messages, options) {
    const startTime = Date.now();

    try {
      // Convert OpenAI format messages to Gemini format
      const systemMessage = messages.find(m => m.role === 'system')?.content || '';
      const userMessages = messages.filter(m => m.role !== 'system');

      // Gemini uses "contents" array with "parts"
      const contents = userMessages.map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      }));

      const requestData = {
        contents: contents,
        generationConfig: {
          temperature: options.temperature || 0.7,
          maxOutputTokens: options.maxTokens || 2048,
          topP: options.topP || 0.95,
          topK: options.topK || 40
        }
      };

      // Add system instruction if available
      if (systemMessage) {
        requestData.systemInstruction = {
          parts: [{ text: systemMessage }]
        };
      }

      const model = options.model || provider.models.chat;
      const url = `${provider.config.baseURL}/models/${model}:streamGenerateContent?key=${provider.config.apiKey}&alt=sse`;

      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestData),
        signal: options.signal
      });

      if (!response.ok) {
        const errorText = await response.text();
        const error = new Error(`Gemini Stream Error: ${response.status} ${response.statusText} - ${errorText}`);
        throw error;
      }

      // Return readable stream that can be consumed by the route handler
      return response.body;
    } catch (error) {
      logger.error('Gemini Stream Error:', {
        error: error.message,
        stack: error.stack,
        model: options.model || provider.models.chat,
        messageCount: messages.length
      });
      throw new Error(`Gemini Stream Error: ${error.message}`);
    }
  }

  // Novel-specific AI methods (Enhanced with Memory)
  async generateResponse(novelContext, userMessage, type = 'general', options = {}) {
    const startTime = Date.now();

    try {
      const locale = normalizeLocale(options.locale);

      // 1. æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰ç”¨æˆ·IDï¼‰
      let memories = [];
      if (options.userId) {
        memories = await memoryService.retrieveRelevantMemories(userMessage, {
          userId: options.userId,
          novelId: novelContext?.id,
          mode: type,
          messageType: options.messageType
        });
      }

      // 2. æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯
      const systemPrompt = memories.length > 0
        ? this.buildMemoryEnhancedPrompt(novelContext, type, memories, locale)
        : this.buildSystemPrompt(novelContext, type, locale);

      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userMessage }
      ];

      // 3. è°ƒç”¨AIæ¨¡å‹
      const response = await this.chat(messages, {
        temperature: options.temperature || (type === 'creative' ? 0.9 : 0.7),
        provider: options.provider,
        model: options.model
      });

      // 4. è§£æå“åº”
      const parsedResponse = this.parseResponse(response.content, type, novelContext, locale);
      const metadata = {
        ...parsedResponse.metadata,
        memoriesUsed: memories.length,
        memoryEnhanced: memories.length > 0,
        processingTime: Date.now() - startTime,
        language: locale
      };

      // 5. å¼‚æ­¥æ›´æ–°è®°å¿†ï¼ˆä¸é˜»å¡å“åº”ï¼‰
      if (options.userId) {
        this.updateMemoriesAsync(userMessage, response.content, {
          userId: options.userId,
          novelId: novelContext?.id,
          mode: type,
          messageType: options.messageType,
          locale
        });
      }

      // 6. è®°å½•æ€§èƒ½æŒ‡æ ‡
      const duration = metadata.processingTime;
      logger.info(`AI Response with Memory: ${duration}ms, memories used: ${memories.length}`);

      return {
        ...parsedResponse,
        metadata
      };

    } catch (error) {
      logger.error('Memory-enhanced AI generation failed:', error);
      // é™çº§åˆ°æ— è®°å¿†æ¨¡å¼
      return await this.generateResponseFallback(novelContext, userMessage, type, options);
    }
  }

  // åŸæœ‰æ–¹æ³•ä½œä¸ºé™çº§æ–¹æ¡ˆ
  async generateResponseFallback(novelContext, userMessage, type = 'general', options = {}) {
    logger.info('Using fallback mode (no memory enhancement)');

    const locale = normalizeLocale(options.locale);
    const startTime = Date.now();
    const systemPrompt = this.buildSystemPrompt(novelContext, type, locale);

    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userMessage }
    ];

    const response = await this.chat(messages, {
      temperature: options.temperature || (type === 'creative' ? 0.9 : 0.7),
      provider: options.provider,
      model: options.model
    });

    const parsedResponse = this.parseResponse(response.content, type, novelContext, locale);
    const processingTime = Date.now() - startTime;
    return {
      ...parsedResponse,
      metadata: {
        ...parsedResponse.metadata,
        language: locale,
        memoriesUsed: 0,
        memoryEnhanced: false,
        processingTime
      }
    };
  }

  // æ„å»ºè®°å¿†å¢å¼ºçš„æç¤ºè¯
  buildMemoryEnhancedPrompt(novelContext, type, memories, locale = DEFAULT_LOCALE) {
    const config = getLocaleConfig(locale);
    let basePrompt = this.buildSystemPrompt(novelContext, type, locale);

    if (memories && memories.length > 0) {
      basePrompt += `\n\n${config.memory.heading}\n`;
      basePrompt += `${config.memory.intro}\n`;

      memories.forEach((memory, index) => {
        basePrompt += `\n${index + 1}. ${memory.content}`;
        if (memory.metadata?.memory_type) {
          basePrompt += ` [${config.memory.typeLabel}: ${memory.metadata.memory_type}]`;
        }
        if (memory.metadata?.importance > 3) {
          basePrompt += ` [${config.memory.importantTag}]`;
        }
      });

      basePrompt += `\n\n${config.memory.closing}`;
      basePrompt += `\n${config.memory.conflictNotice}`;
    }

    return basePrompt;
  }

  // å¼‚æ­¥æ›´æ–°è®°å¿†
  async updateMemoriesAsync(userMessage, aiResponse, context) {
    try {
      // æå–é‡è¦ä¿¡æ¯è¿›è¡Œè®°å¿†
      const importantInfo = this.extractImportantInformation(userMessage, aiResponse, context);

      if (importantInfo.length > 0) {
        // ä½¿ç”¨æ‰¹é‡æ·»åŠ ä»¥é¿å…é˜»å¡
        await memoryService.addMemoryBatch(
          importantInfo.map(info => ({
            content: info.content,
            context: context,
            metadata: {
              memory_type: info.type,
              confidence: info.confidence,
              source: 'ai_conversation',
              extractedFrom: 'ai_response'
            }
          }))
        );
      }

      // å¦‚æœç”¨æˆ·æ¶ˆæ¯åŒ…å«æ˜ç¡®çš„åå¥½è¡¨è¾¾ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
      if (this.containsUserPreference(userMessage)) {
        await memoryService.addMemoryBatch([{
          content: `ç”¨æˆ·åå¥½è¡¨è¾¾: ${userMessage}`,
          context: context,
          metadata: {
            memory_type: 'user_preference',
            confidence: 0.9,
            source: 'user_message'
          }
        }]);
      }

    } catch (error) {
      logger.error('Memory update failed:', error);
    }
  }

  // æå–é‡è¦ä¿¡æ¯
  extractImportantInformation(userMessage, aiResponse, context) {
    const importantInfo = [];

    // è§’è‰²ç›¸å…³ä¿¡æ¯æå–
    const characterPatterns = [
      /(?:è§’è‰²|äººç‰©)[^ã€‚]*?([^ã€‚]{10,})/g,
      /(?:æ€§æ ¼|ç‰¹å¾|èƒŒæ™¯)[^ã€‚]*?([^ã€‚]{10,})/g,
      /(?:ä»–|å¥¹|å®ƒ)(?:æ˜¯|ä¼š|èƒ½)[^ã€‚]*?([^ã€‚]{10,})/g
    ];

    characterPatterns.forEach(pattern => {
      const matches = aiResponse.match(pattern);
      if (matches) {
        matches.forEach(match => {
          importantInfo.push({
            content: match.trim(),
            type: 'character_trait',
            confidence: 0.8
          });
        });
      }
    });

    // ä¸–ç•Œè®¾å®šç›¸å…³ä¿¡æ¯
    const settingPatterns = [
      /(?:ä¸–ç•Œ|è®¾å®š|è§„åˆ™)[^ã€‚]*?([^ã€‚]{10,})/g,
      /(?:åœ°ç‚¹|ä½ç½®|ç¯å¢ƒ)[^ã€‚]*?([^ã€‚]{10,})/g,
      /(?:æ–‡åŒ–|ä¼ ç»Ÿ|ä¹ ä¿—)[^ã€‚]*?([^ã€‚]{10,})/g
    ];

    settingPatterns.forEach(pattern => {
      const matches = aiResponse.match(pattern);
      if (matches) {
        matches.forEach(match => {
          importantInfo.push({
            content: match.trim(),
            type: 'world_setting',
            confidence: 0.8
          });
        });
      }
    });

    // åˆ›ä½œå†³ç­–
    const decisionPatterns = [
      /(?:å»ºè®®|æ¨è|åº”è¯¥)[^ã€‚]*?([^ã€‚]{10,})/g,
      /(?:å¯ä»¥|èƒ½å¤Ÿ|ä¸å¦¨)[^ã€‚]*?([^ã€‚]{10,})/g
    ];

    decisionPatterns.forEach(pattern => {
      const matches = aiResponse.match(pattern);
      if (matches) {
        matches.forEach(match => {
          importantInfo.push({
            content: match.trim(),
            type: 'creative_decision',
            confidence: 0.6
          });
        });
      }
    });

    // ä¸€è‡´æ€§è§„åˆ™
    if (context.mode === 'check' || aiResponse.includes('ä¸€è‡´æ€§') || aiResponse.includes('çŸ›ç›¾')) {
      const consistencyPatterns = [
        /(?:éœ€è¦æ³¨æ„|è¦é¿å…|åº”è¯¥ä¿æŒ)[^ã€‚]*?([^ã€‚]{10,})/g,
        /(?:ä¸€è‡´æ€§|çŸ›ç›¾)[^ã€‚]*?([^ã€‚]{10,})/g
      ];

      consistencyPatterns.forEach(pattern => {
        const matches = aiResponse.match(pattern);
        if (matches) {
          matches.forEach(match => {
            importantInfo.push({
              content: match.trim(),
              type: 'consistency_rule',
              confidence: 0.9
            });
          });
        }
      });
    }

    // å»é‡å’Œè´¨é‡è¿‡æ»¤
    const uniqueInfo = [];
    const seenContent = new Set();

    importantInfo.forEach(info => {
      const normalized = info.content.toLowerCase().replace(/\s+/g, '');
      if (!seenContent.has(normalized) && info.content.length >= 10) {
        seenContent.add(normalized);
        uniqueInfo.push(info);
      }
    });

    return uniqueInfo.slice(0, 5); // é™åˆ¶æ•°é‡é¿å…è¿‡å¤šè®°å¿†
  }

  // æ£€æµ‹ç”¨æˆ·åå¥½è¡¨è¾¾
  containsUserPreference(userMessage) {
    const preferenceKeywords = [
      'å–œæ¬¢', 'ä¸å–œæ¬¢', 'å¸Œæœ›', 'ä¸å¸Œæœ›', 'æƒ³è¦', 'ä¸æƒ³è¦',
      'åå¥½', 'å€¾å‘äº', 'é¿å…', 'æ€»æ˜¯', 'ä»ä¸', 'æ°¸è¿œ'
    ];

    return preferenceKeywords.some(keyword => userMessage.includes(keyword));
  }

  async generateResponseStream(novelContext, userMessage, type = 'general', options = {}) {
    const startTime = Date.now();

    try {
      const locale = normalizeLocale(options.locale);
      // 1. æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰ç”¨æˆ·IDï¼‰
      let memories = [];
      if (options.userId) {
        memories = await memoryService.retrieveRelevantMemories(userMessage, {
          userId: options.userId,
          novelId: novelContext?.id,
          mode: type,
          messageType: options.messageType
        });
      }

      // 2. æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯
      const systemPrompt = memories.length > 0
        ? this.buildMemoryEnhancedPrompt(novelContext, type, memories, locale)
        : this.buildSystemPrompt(novelContext, type, locale);

      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userMessage }
      ];

      // 3. åˆ›å»ºæµå¼å“åº”
      const stream = await this.chatStream(messages, {
        temperature: options.temperature || (type === 'creative' ? 0.9 : 0.7),
        provider: options.provider,
        model: options.model
      });

      // 4. åŒ…è£…æµå¼å“åº”ä»¥æ”¯æŒè®°å¿†æ›´æ–°
      return this.wrapStreamWithMemoryUpdate(stream, {
        userMessage,
        context: {
          userId: options.userId,
          novelId: novelContext?.id,
          mode: type,
          messageType: options.messageType,
          locale
        },
        memoriesUsed: memories.length,
        startTime
      });

    } catch (error) {
      logger.error('Memory-enhanced streaming failed:', error);
      // é™çº§åˆ°æ— è®°å¿†æ¨¡å¼
      return await this.generateResponseStreamFallback(novelContext, userMessage, type, options);
    }
  }

  // é™çº§æ–¹æ¡ˆï¼šæ— è®°å¿†çš„æµå¼å“åº”
  async generateResponseStreamFallback(novelContext, userMessage, type = 'general', options = {}) {
    logger.info('Using fallback mode for streaming (no memory enhancement)');

    const locale = normalizeLocale(options.locale);
    const systemPrompt = this.buildSystemPrompt(novelContext, type, locale);

    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userMessage }
    ];

    return await this.chatStream(messages, {
      temperature: options.temperature || (type === 'creative' ? 0.9 : 0.7),
      provider: options.provider,
      model: options.model
    });
  }

  /**
   * åŒ…è£…æµå¼å“åº”ä»¥æ”¯æŒè®°å¿†æ›´æ–°
   */
  wrapStreamWithMemoryUpdate(originalStream, memoryContext) {
    const { userMessage, context, memoriesUsed, startTime } = memoryContext;
    let fullResponse = '';

    // åˆ›å»ºä¸€ä¸ªæ–°çš„å¼‚æ­¥ç”Ÿæˆå™¨
    const wrappedStream = async function* () {
      try {
        // æµå¼ä¼ é€’åŸå§‹å“åº”
        for await (const chunk of originalStream) {
          // æ”¶é›†å®Œæ•´å“åº”å†…å®¹
          const choice = chunk.choices?.[0];
          if (choice?.delta?.content) {
            fullResponse += choice.delta.content;
          }

          // åŸæ ·ä¼ é€’chunk
          yield chunk;

          // æ£€æµ‹åˆ°æµå¼å“åº”ç»“æŸ
          if (choice?.finish_reason) {
            // å¼‚æ­¥æ›´æ–°è®°å¿†ï¼Œä¸é˜»å¡å“åº”
            if (context.userId && fullResponse.trim()) {
              setImmediate(async () => {
                try {
                  await aiService.updateMemoriesAsync(userMessage, fullResponse, context);

                  // è®°å½•æ€§èƒ½æŒ‡æ ‡
                  const duration = Date.now() - startTime;
                  logger.info(`Streaming response with memory: ${duration}ms, memories used: ${memoriesUsed}`);
                } catch (error) {
                  logger.error('Failed to update memories after streaming:', error);
                }
              });
            }
          }
        }
      } catch (error) {
        logger.error('Error in wrapped stream:', error);
        throw error;
      }
    };

    return wrappedStream();
  }

  buildSystemPrompt(novelContext, type, locale = DEFAULT_LOCALE) {
    const config = getLocaleConfig(locale);
    let basePrompt = config.intro;

    if (novelContext) {
      basePrompt += `\n${config.novelInfoHeading}`;

      if (novelContext.title) {
        basePrompt += `\n${config.titleLabel}${novelContext.title}`;
      }
      if (novelContext.description) {
        basePrompt += `\n${config.descriptionLabel}${novelContext.description}`;
      }
      if (novelContext.genre) {
        basePrompt += `\n${config.genreLabel}${novelContext.genre}`;
      }

      if (novelContext.characters?.length > 0) {
        const characterLines = novelContext.characters
          .map(c => `- ${c.name}${c.description ? `: ${c.description}` : ''}`)
          .join('\n');
        basePrompt += `\n${config.mainCharactersHeading}\n${characterLines}`;
      }

      if (novelContext.settings?.length > 0) {
        const settingLines = novelContext.settings
          .map(s => `- ${s.name}${s.description ? `: ${s.description}` : ''}`)
          .join('\n');
        basePrompt += `\n${config.worldSettingsHeading}\n${settingLines}`;
      }

      if (novelContext.aiSettings?.rating) {
        basePrompt += `\n${config.contentRestrictionsLabel}${novelContext.aiSettings.rating}`;
      }
    }

    const typeInstruction = config.typeGuidance?.[type] || config.typeGuidance?.default;
    if (typeInstruction) {
      basePrompt += `\n${typeInstruction}`;
    }

    if (config.responseDirective) {
      basePrompt += `\n${config.responseDirective}`;
    }

    return basePrompt;
  }

  parseResponse(content, type, novelContext = null, locale = DEFAULT_LOCALE) {
    const normalizedLocale = normalizeLocale(locale);
    // Enhanced response structure
    const result = {
      message: content,
      suggestions: [],
      questions: [],
      actions: [],
      metadata: {
        type,
        timestamp: new Date().toISOString(),
        wordCount: content.length,
        hasStructuredContent: false
      }
    };
    result.metadata.language = normalizedLocale;

    // Extract structured information with multiple patterns
    const suggestionMarkers = [
      'å»ºè®®ï¼š',
      'å»ºè®®:',
      'ğŸ’¡',
      'âœ…',
      'æ¨èï¼š',
      'æ¨è:',
      'Suggestion:',
      'Suggestions:',
      'Tip:',
      'Tips:',
      'Recommendation:',
      'Recommendations:'
    ];
    const questionMarkers = [
      'é—®é¢˜ï¼š',
      'é—®é¢˜:',
      'â“',
      'ğŸ¤”',
      'éœ€è¦è€ƒè™‘ï¼š',
      'éœ€è¦è€ƒè™‘:',
      'Question:',
      'Questions:',
      'Consider:',
      'Considerations:',
      'Open question:',
      'Reflection:'
    ];
    const actionMarkers = [
      'ä¸‹ä¸€æ­¥ï¼š',
      'ä¸‹ä¸€æ­¥:',
      'ğŸ¯',
      'âš¡',
      'è¡ŒåŠ¨å»ºè®®ï¼š',
      'è¡ŒåŠ¨å»ºè®®:',
      'Next step:',
      'Next steps:',
      'Next Step:',
      'Next Steps:',
      'Action:',
      'Actions:',
      'Recommended action:',
      'Recommended actions:'
    ];

    result.suggestions = this.extractBulletPoints(content, suggestionMarkers);
    result.questions = this.extractBulletPoints(content, questionMarkers);
    result.actions = this.extractBulletPoints(content, actionMarkers);

    // Detect if response has structured content
    result.metadata.hasStructuredContent =
      result.suggestions.length > 0 ||
      result.questions.length > 0 ||
      result.actions.length > 0 ||
      content.includes('**') ||
      content.includes('â€¢') ||
      content.includes('1.') ||
      content.includes('##');

    // Add type-specific parsing
    switch (type) {
      case 'consistency':
        result.issues = this.extractConsistencyIssues(content);
        break;
      case 'character':
        result.characterTraits = this.extractCharacterTraits(content);
        break;
      case 'worldbuilding':
        result.worldElements = this.extractWorldElements(content);
        break;
      case 'outline':
        result.plotPoints = this.extractPlotPoints(content);
        break;
    }

    // Generate follow-up suggestions based on content
    result.followUps = this.generateFollowUps(content, type, novelContext, normalizedLocale);

    return result;
  }

  extractConsistencyIssues(content) {
    const issues = [];
    const issuePatterns = [
      /âŒ\s*(.+)/g,
      /âš ï¸\s*(.+)/g,
      /ğŸ”´\s*(.+)/g,
      /é—®é¢˜[ï¼š:]\s*(.+)/g
    ];

    for (const pattern of issuePatterns) {
      let match;
      while ((match = pattern.exec(content)) !== null) {
        issues.push(match[1].trim());
      }
    }

    return issues;
  }

  extractCharacterTraits(content) {
    const traits = [];
    const traitPatterns = [
      /æ€§æ ¼[ï¼š:]\s*(.+)/g,
      /ç‰¹å¾[ï¼š:]\s*(.+)/g,
      /ç‰¹ç‚¹[ï¼š:]\s*(.+)/g
    ];

    for (const pattern of traitPatterns) {
      let match;
      while ((match = pattern.exec(content)) !== null) {
        traits.push(match[1].trim());
      }
    }

    return traits;
  }

  extractWorldElements(content) {
    const elements = [];
    const elementPatterns = [
      /ğŸ›ï¸\s*(.+)/g,
      /ğŸŒ\s*(.+)/g,
      /è®¾å®š[ï¼š:]\s*(.+)/g
    ];

    for (const pattern of elementPatterns) {
      let match;
      while ((match = pattern.exec(content)) !== null) {
        elements.push(match[1].trim());
      }
    }

    return elements;
  }

  extractPlotPoints(content) {
    const points = [];
    const pointPatterns = [
      /ğŸ“Š\s*(.+)/g,
      /æƒ…èŠ‚[ï¼š:]\s*(.+)/g,
      /ç¬¬\d+ç« [ï¼š:]\s*(.+)/g
    ];

    for (const pattern of pointPatterns) {
      let match;
      while ((match = pattern.exec(content)) !== null) {
        points.push(match[1].trim());
      }
    }

    return points;
  }

  generateFollowUps(content, type, novelContext, locale = DEFAULT_LOCALE) {
    const normalizedLocale = normalizeLocale(locale);
    const followUps = [];
    const text = content || '';
    const lowerText = text.toLowerCase();

    const isCharacterTopic =
      type === 'character' ||
      text.includes('è§’è‰²') ||
      text.includes('æ€§æ ¼') ||
      lowerText.includes('character') ||
      lowerText.includes('personality');

    if (isCharacterTopic) {
      if (normalizedLocale === 'en') {
        followUps.push("Should I expand this character's backstory even further?");
        followUps.push("Do you want suggestions for this character's dialogue style or traits?");
      } else {
        followUps.push('è¦ä¸è¦ç»§ç»­å®Œå–„è¿™ä¸ªè§’è‰²çš„èƒŒæ™¯æ•…äº‹ï¼Ÿ');
        followUps.push('éœ€è¦ä¸ºè¿™ä¸ªè§’è‰²è®¾è®¡ä¸€äº›å…·ä½“çš„å¯¹è¯é£æ ¼å—ï¼Ÿ');
      }
    }

    const isWorldbuildingTopic =
      type === 'worldbuilding' ||
      text.includes('è®¾å®š') ||
      text.includes('ä¸–ç•Œ') ||
      lowerText.includes('world') ||
      lowerText.includes('setting') ||
      lowerText.includes('lore');

    if (isWorldbuildingTopic) {
      if (normalizedLocale === 'en') {
        followUps.push("Would you like me to expand the world's history or lore?");
        followUps.push('Should we define more rules or constraints for this setting?');
      } else {
        followUps.push('éœ€è¦è¿›ä¸€æ­¥æ‰©å±•è¿™ä¸ªä¸–ç•Œçš„å†å²èƒŒæ™¯å—ï¼Ÿ');
        followUps.push('è¦ä¸ºè¿™ä¸ªè®¾å®šæ·»åŠ ä¸€äº›å…·ä½“çš„è§„åˆ™é™åˆ¶å—ï¼Ÿ');
      }
    }

    const mentionsConsistency =
      type === 'consistency' ||
      text.includes('é—®é¢˜') ||
      text.includes('çŸ›ç›¾') ||
      lowerText.includes('issue') ||
      lowerText.includes('conflict') ||
      lowerText.includes('inconsistency');

    if (mentionsConsistency) {
      if (normalizedLocale === 'en') {
        followUps.push('Would you like a concrete plan to resolve these issues?');
        followUps.push('Should I check other chapters for similar inconsistencies?');
      } else {
        followUps.push('è¦æˆ‘å¸®ä½ åˆ¶å®šä¿®å¤è¿™äº›é—®é¢˜çš„å…·ä½“æ–¹æ¡ˆå—ï¼Ÿ');
        followUps.push('éœ€è¦æ£€æŸ¥å…¶ä»–ç« èŠ‚æ˜¯å¦æœ‰ç±»ä¼¼é—®é¢˜å—ï¼Ÿ');
      }
    }

    return followUps.slice(0, 3); // Limit to 3 follow-ups
  }

  extractBulletPoints(text, markers) {
    const points = [];
    const uniquePoints = new Set();

    for (const marker of markers) {
      const regex = new RegExp(`${marker.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')}([\\s\\S]*?)(?=\\n\\n|\\n[^\\nâ€¢\\-\\d]|$)`, 'g');
      let match;

      while ((match = regex.exec(text)) !== null) {
        const section = match[1];
        const lines = section.split('\n');

        for (const line of lines) {
          const trimmed = line.trim();
          const bulletPatterns = [
            /^[-â€¢âœ“âœ…]\s*(.+)$/,
            /^\d+\.\s*(.+)$/,
            /^[a-zA-Z]\)\s*(.+)$/,
            /^[\u4e00-\u9fff]ã€\s*(.+)$/
          ];

          for (const pattern of bulletPatterns) {
            const bulletMatch = trimmed.match(pattern);
            if (bulletMatch && bulletMatch[1]) {
              const point = bulletMatch[1].trim();
              if (point.length > 5 && !uniquePoints.has(point)) {
                uniquePoints.add(point);
                points.push(point);
              }
              break;
            }
          }
        }
      }
    }

    return points.slice(0, 10); // Limit to avoid too many points
  }

  async checkConsistency(novelData, scope = 'full') {
    const startTime = Date.now();

    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´ä¸€è‡´æ€§æ£€æŸ¥åŠ©æ‰‹ã€‚è¯·ä»”ç»†æ£€æŸ¥ä»¥ä¸‹å†…å®¹ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ï¼š

æ£€æŸ¥é‡ç‚¹ï¼š
1. è§’è‰²æ€§æ ¼å’Œè¡Œä¸ºçš„ä¸€è‡´æ€§
2. ä¸–ç•Œè®¾å®šçš„é€»è¾‘ä¸€è‡´æ€§
3. æ—¶é—´çº¿çš„åˆç†æ€§
4. æƒ…èŠ‚å‘å±•çš„è¿è´¯æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€æŸ¥ç»“æœï¼ŒåŒ…å«ï¼š
- issues: é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å« type(ç±»å‹), severity(ä¸¥é‡ç¨‹åº¦), issue(é—®é¢˜æè¿°), suggestion(ä¿®æ”¹å»ºè®®)
- summary: é—®é¢˜ç»Ÿè®¡æ‘˜è¦`;

    const messages = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: this.formatNovelDataForConsistency(novelData) }
    ];

    try {
      const response = await this.chat(messages, {
        temperature: 0.3, // Lower temperature for consistency checking
        taskType: 'consistency'
      });

      let result;
      try {
        // Try to parse as JSON first
        result = JSON.parse(response.content);
      } catch (parseError) {
        // If JSON parsing fails, try to extract structured info from text
        result = this.parseConsistencyFromText(response.content);
      }

      // Ensure result has required structure
      if (!result.issues) result.issues = [];
      if (!result.summary) {
        const issues = result.issues;
        result.summary = {
          total: issues.length,
          high: issues.filter(i => i.severity === 'high').length,
          medium: issues.filter(i => i.severity === 'medium').length,
          low: issues.filter(i => i.severity === 'low').length
        };
      }

      // Log consistency check
      const duration = Date.now() - startTime;
      logger.logConsistencyCheck(novelData.id, scope, result.issues || [], duration);

      return result;
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error('Consistency check failed:', {
        novelId: novelData.id,
        scope,
        error: error.message,
        duration: `${duration}ms`
      });

      // Enhanced fallback response
      return {
        issues: [{
          type: 'system',
          severity: 'low',
          issue: 'AIä¸€è‡´æ€§æ£€æŸ¥æœåŠ¡æš‚æ—¶ä¸å¯ç”¨',
          suggestion: 'è¯·ç¨åé‡è¯•ï¼Œæˆ–æ‰‹åŠ¨æ£€æŸ¥å†…å®¹ä¸€è‡´æ€§ã€‚æ‚¨å¯ä»¥é‡ç‚¹å…³æ³¨è§’è‰²è¡Œä¸ºã€æ—¶é—´çº¿å’Œä¸–ç•Œè®¾å®šçš„è¿è´¯æ€§ã€‚'
        }],
        summary: { total: 1, high: 0, medium: 0, low: 1 }
      };
    }
  }

  parseConsistencyFromText(content) {
    const issues = [];
    const lines = content.split('\n');

    for (const line of lines) {
      const trimmed = line.trim();

      // Look for issue indicators
      if (trimmed.includes('âŒ') || trimmed.includes('é—®é¢˜') || trimmed.includes('çŸ›ç›¾')) {
        issues.push({
          type: 'consistency',
          severity: 'medium',
          issue: trimmed.replace(/[âŒâš ï¸ğŸ”´]/g, '').trim(),
          suggestion: 'è¯·æ£€æŸ¥å¹¶ä¿®æ­£æ­¤ä¸€è‡´æ€§é—®é¢˜'
        });
      } else if (trimmed.includes('âš ï¸') || trimmed.includes('æ³¨æ„')) {
        issues.push({
          type: 'warning',
          severity: 'low',
          issue: trimmed.replace(/[âŒâš ï¸ğŸ”´]/g, '').trim(),
          suggestion: 'å»ºè®®è¿›ä¸€æ­¥ç¡®è®¤æ­¤å¤„å†…å®¹'
        });
      }
    }

    return {
      issues,
      summary: {
        total: issues.length,
        high: 0,
        medium: issues.filter(i => i.severity === 'medium').length,
        low: issues.filter(i => i.severity === 'low').length
      },
      rawContent: content
    };
  }

  formatNovelDataForConsistency(novelData) {
    let formatted = `å°è¯´ï¼š${novelData.title}\n`;
    
    if (novelData.characters?.length > 0) {
      formatted += '\nè§’è‰²ä¿¡æ¯ï¼š\n';
      novelData.characters.forEach(char => {
        formatted += `${char.name}: ${char.description}\næ€§æ ¼ï¼š${char.personality || 'æœªè®¾å®š'}\nèƒŒæ™¯ï¼š${char.background || 'æœªè®¾å®š'}\n\n`;
      });
    }

    if (novelData.chapters?.length > 0) {
      formatted += '\nç« èŠ‚å†…å®¹ï¼š\n';
      novelData.chapters.forEach(chapter => {
        formatted += `ç¬¬${chapter.chapterNumber}ç«  - ${chapter.title}\n`;
        if (chapter.outline) formatted += `å¤§çº²ï¼š${chapter.outline}\n`;
        if (chapter.content) formatted += `å†…å®¹ï¼š${chapter.content.substring(0, 500)}...\n`;
        formatted += '\n';
      });
    }

    return formatted.substring(0, 8000); // Limit context size to avoid token limits
  }

  // New method for enhanced conversation support
  async generateConversationalResponse(novelContext, userMessage, conversationHistory = [], options = {}) {
    const systemPrompt = this.buildConversationalPrompt(novelContext, options.mode);

    const messages = [{ role: 'system', content: systemPrompt }];

    // Add relevant conversation history
    if (conversationHistory.length > 0) {
      const recentHistory = conversationHistory.slice(-8); // Last 8 messages
      messages.push(...recentHistory.map(msg => ({
        role: msg.role,
        content: msg.content
      })));
    }

    messages.push({ role: 'user', content: userMessage });

    const response = await this.chat(messages, {
      temperature: 0.8, // Slightly higher for natural conversation
      provider: options.provider,
      model: options.model
    });

    return {
      content: response.content,
      suggestions: this.extractBulletPoints(response.content, ['å»ºè®®ï¼š', 'å»ºè®®:', 'ğŸ’¡']),
      followUps: this.generateFollowUps(response.content, 'general', novelContext),
      metadata: {
        provider: response.provider,
        model: response.model,
        timestamp: new Date().toISOString()
      }
    };
  }

  buildConversationalPrompt(novelContext, mode = 'chat') {
    let prompt = 'ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å°è¯´åˆ›ä½œå¯¼å¸ˆï¼Œå…·æœ‰æ·±åšçš„æ–‡å­¦åŠŸåº•å’Œä¸°å¯Œçš„åˆ›ä½œç»éªŒã€‚è¯·ä»¥å‹å¥½ã€ä¸“ä¸šçš„æ€åº¦ä¸ç”¨æˆ·å¯¹è¯ã€‚';

    if (mode === 'enhance') {
      prompt += 'å½“å‰ä¸“æ³¨äºå¸®åŠ©ç”¨æˆ·å®Œå–„åˆ›ä½œå†…å®¹ï¼ŒåŒ…æ‹¬è§’è‰²å‘å±•ã€æƒ…èŠ‚è®¾è®¡å’Œä¸–ç•Œæ„å»ºã€‚';
    } else if (mode === 'check') {
      prompt += 'å½“å‰ä¸“æ³¨äºå¸®åŠ©ç”¨æˆ·è¿›è¡Œè´¨é‡æ£€æŸ¥ï¼ŒåŒ…æ‹¬ä¸€è‡´æ€§åˆ†æå’Œé€»è¾‘å®¡æ ¸ã€‚';
    } else {
      prompt += 'å½“å‰å¤„äºè‡ªç”±å¯¹è¯æ¨¡å¼ï¼Œå¯ä»¥è®¨è®ºä»»ä½•ä¸åˆ›ä½œç›¸å…³çš„è¯é¢˜ã€‚';
    }

    if (novelContext) {
      prompt += `\n\nå½“å‰è®¨è®ºçš„å°è¯´ï¼šã€Š${novelContext.title}ã€‹`;
      if (novelContext.description) {
        prompt += `\nç®€ä»‹ï¼š${novelContext.description.substring(0, 200)}...`;
      }
    }

    prompt += `\n\nè¯·æ³¨æ„ï¼š
â€¢ ä¿æŒå¯¹è¯çš„è¿ç»­æ€§å’Œä¸Šä¸‹æ–‡ç†è§£
â€¢ æä¾›å…·ä½“ã€å¯è¡Œçš„å»ºè®®
â€¢ é€‚æ—¶æå‡ºå¼•å¯¼æ€§é—®é¢˜
â€¢ é¼“åŠ±ç”¨æˆ·çš„åˆ›ä½œçƒ­æƒ…
â€¢ ç”¨æ¸©æš–ã€ä¸“ä¸šçš„è¯­è°ƒå›åº”`;

    return prompt;
  }
}

module.exports = new AIService();
